from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from app.database import get_db
from app.models import ChatHistory, User
from pydantic import BaseModel
import requests
import os
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

class ChatMessage(BaseModel):
    user_id: int
    message: str

router = APIRouter(prefix="/chat", tags=["Chatbot"])

# --- AI Configuration ---
# 1. Ollama (Local)
OLLAMA_URL = "http://localhost:11434/api/generate"

# 2. Gemini (Cloud Fallback for Render/Vercel)
GEMINI_KEY = os.getenv("GEMINI_API_KEY", "AIzaSyDBcjr_LUjxRP627desuRKgLo7M0bwPqZQ")
genai.configure(api_key=GEMINI_KEY)
gemini_model = genai.GenerativeModel('gemini-1.5-flash')

def get_ai_response(prompt, user_name):
    full_prompt = f"You are a friendly librarian assistant. Address the user as {user_name}. Keep it brief (2 sentences). Question: {prompt}"
    
    # Try Ollama first (Local/Development)
    try:
        print(f"ü¶ô Attempting to use Ollama...")
        response = requests.post(
            OLLAMA_URL,
            json={"model": "llama3", "prompt": full_prompt, "stream": False},
            timeout=30  # Increased timeout for local hardware
        )
        if response.status_code == 200:
            print("‚úÖ Response generated by Ollama!")
            return response.json().get("response")
    except Exception as e:
        print(f"‚ö†Ô∏è Ollama unavailable, switching to Gemini. Error: {e}")

    # Fallback to Gemini (Production/Cloud)
    try:
        response = gemini_model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        error_msg = str(e)
        if "429" in error_msg:
            return "üòÖ I'm a bit overwhelmed with requests right now. Please try again in a minute!"
        return "üîß AI service is temporarily unavailable. Please check back later!"

@router.post("/")
def chat_with_bot(chat_data: ChatMessage, db: Session = Depends(get_db)):
    user = db.query(User).filter(User.id == chat_data.user_id).first()
    user_name = user.name if user and user.name else "Library User"
    
    bot_response = get_ai_response(chat_data.message, user_name)
    
    # Save to Database
    new_chat = ChatHistory(
        user_id=chat_data.user_id,
        message=chat_data.message,
        response=bot_response
    )
    db.add(new_chat)
    db.commit()
    
    return {"response": bot_response}

@router.get("/{user_id}/history")
def get_chat_history(user_id: int, db: Session = Depends(get_db)):
    history = db.query(ChatHistory).filter(ChatHistory.user_id == user_id).order_by(ChatHistory.timestamp.desc()).limit(10).all()
    return [{"message": h.message, "response": h.response, "timestamp": h.timestamp} for h in history]

@router.delete("/{user_id}/history")
def clear_history(user_id: int, db: Session = Depends(get_db)):
    db.query(ChatHistory).filter(ChatHistory.user_id == user_id).delete()
    db.commit()
    return {"message": "History cleared"}